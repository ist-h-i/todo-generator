#!/usr/bin/env bash
set -euo pipefail

mkdir -p .github/workflows scripts .codex/auto_evolve templates/roles/new_role docs/metrics docs/auto-evolve codex_output/metrics codex_output/auto_evolve

# ------------- .github/workflows/auto-evolve.yml -------------
cat > .github/workflows/auto-evolve.yml <<'YML'
name: Auto Evolve Roles
on:
  schedule: [{ cron: "0 3 * * 1" }] # JST 月曜 12:00 -> UTC 03:00 目安で運用
  workflow_dispatch: {}

permissions:
  contents: write
  pull-requests: write

  jobs:
    evolve:
      runs-on: ubuntu-latest
      timeout-minutes: 30
      steps:
        - uses: actions/checkout@v4
          with: { fetch-depth: 0 }

        - name: Setup Python 3.11
          uses: actions/setup-python@v5
          with: { python-version: "3.11" }

        - name: Install Python dependencies
          run: |
            python -m pip install --upgrade pip
            pip install PyYAML

        - name: Install deps (jq)
          run: |
            sudo apt-get update -y
            sudo apt-get install -y jq

      - name: Aggregate metrics
        run: |
          chmod +x scripts/_metrics.sh || true
          python3 scripts/collect_metrics.py || true

      - name: Optimize & propose
        run: |
          python3 scripts/role_optimizer.py

      - name: Render evidence (graph/plan/rollback)
        run: |
          python3 scripts/render_role_changes.py --proposals codex_output/auto_evolve/proposals.json

      - name: Create PR
        uses: peter-evans/create-pull-request@v6
        with:
          title: "Auto-Evolve: role consolidation/route optimization proposals"
          branch: auto-evolve/proposal-${{ github.run_id }}
          commit-message: "auto-evolve: policy-driven proposals"
          body: |
            This PR is auto-generated by **Auto-Evolve**.
            - Evidence: `codex_output/auto_evolve/proposals.json`
            - Graph: `codex_output/auto_evolve/graph_before_after.md`
            - A/B plan: `codex_output/auto_evolve/experiment_plan.md`
            - Rollback: `codex_output/auto_evolve/rollback.md`
YML

# ------------- scripts/_metrics.sh -------------
cat > scripts/_metrics.sh <<'SH'
#!/usr/bin/env bash
# Usage (each role tail):
#   append_role_metric "Coder" 1 0.12 0.03 12345 0.7
#                        role   activity  return fail  tokens contrib
set -euo pipefail
mkdir -p codex_output/metrics

append_role_metric() {
  local role="$1"; shift
  local activity="$1"; shift
  local return_rate="$1"; shift
  local fail_rate="$1"; shift
  local tokens="$1"; shift
  local contrib="$1"; shift
  local now
  now="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  jq -n --arg role "$role" --argjson activity "$activity" \
        --argjson return_rate "$return_rate" --argjson fail_rate "$fail_rate" \
        --argjson tokens "$tokens" --argjson contrib "$contrib" \
        --arg timestamp "$now" '{role: $role, activity: $activity, return_rate: $return_rate, fail_rate: $fail_rate, tokens: $tokens, contrib: $contrib, ts: $timestamp}' \
    >> codex_output/metrics/metrics.jsonl
}
SH
chmod +x scripts/_metrics.sh

# ------------- scripts/collect_metrics.py -------------
cat > scripts/collect_metrics.py <<'PY'
from __future__ import annotations
import json, pathlib, statistics as st
from collections import defaultdict

METRICS = pathlib.Path("codex_output/metrics/metrics.jsonl")
OUT_JSON = pathlib.Path("codex_output/metrics/roles.json")


def load_lines():
    if not METRICS.exists():
        return []
    rows = []
    for line in METRICS.read_text(encoding="utf-8").splitlines():
        try:
            rows.append(json.loads(line))
        except Exception:
            pass
    return rows


def aggregate(rows):
    by_role = defaultdict(list)
    for r in rows:
        by_role[r.get("role", "Unknown")].append(r)

    out = []
    for role, items in by_role.items():
        activity = sum(1 for _ in items)
        return_rate = st.mean([i.get("return_rate", 0.0) for i in items]) if items else 0.0
        fail_rate = st.mean([i.get("fail_rate", 0.0) for i in items]) if items else 0.0
        cost = st.mean([i.get("tokens", 0.0) for i in items]) if items else 0.0
        contrib = st.mean([i.get("contrib", 0.0) for i in items]) if items else 0.0
        out.append({
            "role": role,
            "activity": float(activity),
            "return_rate": float(return_rate),
            "fail_rate": float(fail_rate),
            "cost": float(cost),
            "contrib": float(contrib),
            "idle_runs": 0,
            "prompt_embedding": [0.0, 0.0, 0.0],  # placeholder (LLM埋め込みは別途拡張)
        })

    # グローバル失敗パターン（簡易ダミー）
    global_patterns = [{"topic": "i18n_missing", "count": 0}, {"topic": "sbom_gap", "count": 0}]
    return {"roles": out, "global_fail_patterns": global_patterns}


def main():
    rows = load_lines()
    agg = aggregate(rows)
    OUT_JSON.write_text(json.dumps(agg, indent=2), encoding="utf-8")

if __name__ == "__main__":
    main()
PY

# ------------- scripts/role_optimizer.py -------------
cat > scripts/role_optimizer.py <<'PY'
from __future__ import annotations
import json
from pathlib import Path

METRICS = Path("codex_output/metrics/roles.json")
POLICY_YML = Path(".codex/auto_evolve/policy.yml")
OUT = Path("codex_output/auto_evolve/proposals.json")


def load_metrics():
    data = json.loads(METRICS.read_text(encoding="utf-8")) if METRICS.exists() else {"roles": [], "global_fail_patterns": []}
    return data


def load_policy():
    # 最小実装: YAML依存を避け、JSONも許容
    try:
        import yaml
    except ModuleNotFoundError as exc:  # pragma: no cover - defensive
        raise RuntimeError(
            "PyYAML is required to load .codex/auto_evolve/policy.yml. Install it via `pip install PyYAML`."
        ) from exc
    return yaml.safe_load(POLICY_YML.read_text(encoding="utf-8"))


def normalize(val, lo, hi):
    return 0.0 if hi - lo <= 1e-9 else max(0.0, min(1.0, (val - lo) / (hi - lo)))


def health_score(role, stats):
    # health = 0.25*z(activity) + 0.25*(1-return) + 0.2*(1-fail) + 0.2*contrib - 0.1*z(cost)
    z = lambda x, k: normalize(role.get(k, 0.0), stats[k][0], stats[k][1])
    return (
        0.25 * z(role, "activity") +
        0.25 * (1 - role.get("return_rate", 0.0)) +
        0.20 * (1 - role.get("fail_rate", 0.0)) +
        0.20 * role.get("contrib", 0.0) -
        0.10 * z(role, "cost")
    )


def compute_stats(roles):
    def bounds(key):
        vals = [r.get(key, 0.0) for r in roles]
        return (min(vals) if vals else 0.0, max(vals) if vals else 1.0)
    return {k: bounds(k) for k in ("activity", "cost")}


def propose(data, policy):
    roles = data.get("roles", [])
    stats = compute_stats(roles)

    # A) idle detection
    idle = [r for r in roles if r.get("activity", 0.0) == 0.0 or r.get("idle_runs", 0) >= 20]

    # B) consolidation (最小実装: 同名prefixで冗長候補を仮検出)
    redundant_pairs = []
    for i, ri in enumerate(roles):
        for j, rj in enumerate(roles[i+1:], i+1):
            if ri["role"].split()[0] == rj["role"].split()[0]:
                redundant_pairs.append({"a": ri["role"], "b": rj["role"], "reason": "name-prefix-match"})

    # C) gaps
    gaps = [g for g in data.get("global_fail_patterns", []) if g.get("count", 0) >= 5]

    # D) route optimization (ダミー提案)
    routing = {"suggest": "increase_weight:AI_Safety"}

    # health score
    for r in roles:
        r["health"] = health_score(r, stats)

    return {
        "idle": idle,
        "redundant_pairs": redundant_pairs,
        "gaps": gaps,
        "routing": routing,
        "roles": roles,
        "policy": policy,
    }


def main():
    data = load_metrics()
    policy = load_policy()
    OUT.parent.mkdir(parents=True, exist_ok=True)
    OUT.write_text(json.dumps(propose(data, policy), indent=2), encoding="utf-8")

if __name__ == "__main__":
    main()
PY

# ------------- scripts/render_role_changes.py -------------
cat > scripts/render_role_changes.py <<'PY'
from __future__ import annotations
import json, argparse
from pathlib import Path

OUT_DIR = Path("codex_output/auto_evolve")


def mermaid_from_roles(roles):
    nodes = "\n".join([f"  {r['role'].replace(' ', '_')}[\"{r['role']}\\nhealth: {r.get('health', 0):.2f}\"]" for r in roles])
    # エッジは最小ダミー（実装で更新）
    edges = "\n".join(["  Translator --> Planner", "  Planner --> Coder", "  Coder --> Code_Quality_Reviewer", "  Coder --> Security_Reviewer", "  Security_Reviewer --> Integrator", "  Code_Quality_Reviewer --> Integrator", "  Integrator --> Release_Manager", "  Release_Manager --> Doc_Writer", "  Doc_Writer --> Doc_Editor"])
    return f"""flowchart LR\n{nodes}\n{edges}\n"""


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--proposals", required=True)
    args = ap.parse_args()

    data = json.loads(Path(args.proposals).read_text(encoding="utf-8"))
    roles = data.get("roles", [])
    OUT_DIR.mkdir(parents=True, exist_ok=True)

    # Graph before/after（初回は after のみ）
    (OUT_DIR / "graph_before_after.md").write_text(
        "# Workflow Graph (candidate)\n\n```mermaid\n" + mermaid_from_roles(roles) + "```\n",
        encoding="utf-8",
    )

    # Experiment plan
    (OUT_DIR / "experiment_plan.md").write_text(
        """# Experiment Plan\n- Canary: 10% of tasks (flags.json: canary_ratio)\n- Success: CI time -5% OR return_rate -10%, and defect density +0% ~ +10% max\n- Metrics window: 7 days\n- Review gates: keep human approval for disable/delete\n""",
        encoding="utf-8",
    )

    # Rollback
    (OUT_DIR / "rollback.md").write_text(
        """# Rollback\n- Restore `.codex/flags.json` previous commit\n- Re-run guardrails job or dispatch manual rollback\n- Announce in PR with metrics snapshot\n""",
        encoding="utf-8",
    )

if __name__ == "__main__":
    main()
PY

# ------------- scripts/guardrails.py -------------
cat > scripts/guardrails.py <<'PY'
from __future__ import annotations
# 監視→閾値超過で flags 復旧（最小実装のためダミー）
print("guardrails: no regression detected (placeholder)")
PY

# ------------- .codex/auto_evolve/policy.yml -------------
cat > .codex/auto_evolve/policy.yml <<'YML'
min_data_points: 50
health_thresholds:
  warn: 0.35
  merge_candidate: 0.15
allowed_changes:
  - consolidate_roles
  - disable_role
  - create_role_from_gap
  - reroute_edges
require_human_approval:
  - disable_role
  - delete_role
guardrails:
  ab_test_min_duration_runs: 20
  max_parallel_experiments: 2
  rollback_on:
    - ci_regression_over_pct: 5
    - defect_density_increase_pct: 10
YML

# ------------- .codex/flags.json -------------
cat > .codex/flags.json <<'JSON'
{
  "roles": {
    "Security Reviewer": {"enabled": true},
    "Code Quality Reviewer": {"enabled": true},
    "UI/UX Reviewer": {"enabled": true},
    "AI Safety Reviewer": {"enabled": true}
  },
  "route": {
    "weights": {
      "AI_Safety": 1.2,
      "Performance": 1.0,
      "Accessibility": 1.0
    }
  },
  "canary_ratio": 0.1
}
JSON

# ------------- templates/roles/new_role/prompt.md -------------
cat > templates/roles/new_role/prompt.md <<'MD'
# New Role Prompt (template)
- Mission: <fill>
- Inputs: <files/recipes>
- Outputs: workflow/<role>/{timestamp}-{slug}.md with sections: Summary / Steps / Evidence / Recipe Updates / Risks
- Guardrails: follow docs/metrics/schema.md and recipes
MD

# ------------- templates/roles/new_role/checklist.md -------------
cat > templates/roles/new_role/checklist.md <<'MD'
- [ ] Validate inputs against schema
- [ ] Produce Evidence with references
- [ ] Propose recipe updates if recurring
- [ ] Report risks and mitigation
MD

# ------------- templates/roles/new_role/log.md -------------
cat > templates/roles/new_role/log.md <<'MD'
# <Role> Log
## Summary
## Steps
## Evidence & References
## Recipe Updates
## Risks & Follow-ups
MD

# ------------- docs/metrics/schema.md -------------
cat > docs/metrics/schema.md <<'MD'
# Metrics Schema (minimum viable)
- role: string
- activity: number (count)
- return_rate: 0..1 (send-back ratio)
- fail_rate: 0..1 (CI/test failure ratio)
- tokens: number (avg tokens)
- contrib: 0..1 (contribution proxy)
MD

# ------------- docs/auto-evolve/operations.md -------------
cat > docs/auto-evolve/operations.md <<'MD'
# Auto-Evolve Operations
- Dispatch `Auto Evolve Roles` manually to generate a proposal PR
- Review `proposals.json` / graph / plan / rollback in the PR body
- Apply canary via `.codex/flags.json` and monitor metrics dashboard
- If regression, run guardrails or revert flags
MD

chmod +x .github/workflows/auto-evolve.yml || true

echo "Scaffold written. Next: add flags read to codex.yml as below."
